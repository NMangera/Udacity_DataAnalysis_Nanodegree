{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Udacity Data Analysis Nanodegree </center>\n",
    "   ## <center> Project: WeRateDogs Twitter Data </center>\n",
    "   ### <center> Noaman Mangera, July 2020 <center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Archive\n",
    "The Twitter archive of WeRateDogs contains data for 5000+ of tweets. The tweets themselves however, do not contain all the data required to complete the analysis. This is downloaded manually and stored locally. From there, the file is imported into the Python environent using Pandas and read into a dataframe called twitter_archive.\n",
    "\n",
    "### Image Predictions\n",
    "This dataset contains predictions of dog breed from each tweet according to a neural network. The file is downloaded programmatically using python Requests library and saved locally. The file is then read into a dataframe called image_predictions. \n",
    "\n",
    "### Additional Data via the Twitter API\n",
    "Retweet count and favorite count are two notable omissions from the above data. Fortunately, this additional data can be gathered from Twitter's API. Well, \"anyone\" who with a developers account. With tweet IDs used to find matching tweets this piece of data is extracted using the Pytyhon library tweepy and stored and then saved locally in JSON format. It is then read back into Python as a Pandas data frame called df_tweet_json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A manual inspection was first made with the .shape method to determine the number of columns and rows. The .info method was then applied to each dataframe to ascertain an overview of the data types of each column, and the number of null values contained within each. This revealed the assignment of incorrect data types for a number of columns.\n",
    "\n",
    "A visual assessment was also carried out using the .sample method so that a snapshot of reprensetative rows throughout the datasets could be viewed. This revealed columns and values unreadable to the human eye, and as such, required attention.\n",
    "\n",
    "Delving deeper the .describe method calculated summary statistics for each of the numeric columns in the dataframe. Summary statistics such as the mean, standard deviation and quartile information brought much needed clarity to the shape and distribution of the quantitative variables. \n",
    "\n",
    "A user defined function was then written to programmatically count the number of, and % of null values contained within each field within each dataset. No Null values were found in image_predictions or df_tweet_json. Null values related to retweet and reply information were found to be present in the twitter_archive data.\n",
    "\n",
    "Duplicate Tweets were also identified using the .duplicated method. \n",
    "\n",
    "The .value_counts methods highlighted several issues, including the usage of the a place holder labelled 'None' with the field name to indicate missing values.\n",
    "\n",
    "The twitter_archive also stored information relating to the life stage in which a particular dog was at into 4 columns, when it could just as easily have been contained within a single column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before cleaning a copy of the data was made to ensure the original information remained for future reference.\n",
    "\n",
    "The uneccesary 'doggo', 'floofer', 'pupper' and 'puppo' columns were merged into one column by adding the contents to a new column called dog_stage.\n",
    "\n",
    "The three datasets were combined into one using the merge function into a 'master' dataset.\n",
    "\n",
    "Retweets were removed using the .dropna method, while unused columns were dropped using the .drop method.\n",
    "\n",
    "Values in the the source field was simplified to a more human readable format using the string replace function .str.replace.\n",
    "\n",
    "Incorrect datatypes were reassigned to a more appropriate datatype using the .astype method, while innapropriatly labelled column names were renamed using the .rename method.\n",
    "\n",
    "Incorrectly named dog names 'None' were replaced using the replace method.\n",
    "\n",
    "The cleaned dataset was saved to the working directory as 'twitter_archive_master.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
